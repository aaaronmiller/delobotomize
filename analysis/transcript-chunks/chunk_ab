 For each: Do I have an account? Is there a free tier?
- [ ] For each: Have I read the docs or just assumed it works?

## Success Criteria
- [ ] How will I know this project "works"? (Specific test case)
- [ ] What is my acceptable failure rate? (e.g., "90% success is fine")
- [ ] What would make me abandon this project? (Be honest)

## Risk Assessment
- [ ] What is the ONE thing most likely to break?
- [ ] Do I have a backup plan if that breaks?
- [ ] Am I using any "experimental" or "beta" APIs?
Phase 1: First Implementation (Reality Check)
Before Writing "Real" Code:

bash
# Create a scratch.js file and test your assumptions
# Rule: If you can't make it work in 20 lines, you don't understand it yet

# Example: Test Gemini API
node scratch.js # Does it return JSON? Does response_format work?
Validation Script Template:

javascript
// validate-deps.js - Run this ONCE before building
const tests = [
  {
    name: 'API X supports feature Y',
    test: async () => { /* actual API call */ }
  }
];

for (const t of tests) {
  try {
    await t.test();
    console.log(`âœ… ${t.name}`);
  } catch (e) {
    console.log(`âŒ ${t.name}: ${e.message}`);
    console.log('âš ï¸  Adjust your design - this won't work');
  }
}
```

#### **Phase 2: Development (Ongoing Checks)**

**Daily Commit Message Format:**
```
What: Added feature X
Why: Solves problem Y  
Risk: Assumes API Z works (not verified)
Weekly Review Questions:

markdown
- [ ] Did I hardcode anything that will break on another machine?
- [ ] Did I add any dependencies without testing them first?
- [ ] Is there a file/directory this won't work without?
- [ ] What's the worst bug hiding in code I wrote this week?
Phase 3: "Done" Checklist
Before Calling It Complete:

markdown
- [ ] Ran it on a fresh terminal session (not my dev environment)
- [ ] Tested what happens when API is down/slow/returns garbage
- [ ] Documented the 3 most likely failure modes
- [ ] Created a 1-page "how to run this" doc for future me
ğŸ¯ What Ice-ninja Really Needs (Hypothesis)
Based on the notes, Ice-ninja likely needs:

Quick "Smoke Test" Scripts - Not full test suites, just "does this API even work?"
Forcing Functions - Questions that make you pause before assuming
Lightweight Documentation - README-level, not enterprise architecture docs
Failure Pre-Mortems - "What will break?" not "How do we govern decisions?"
Copy-Paste Validation Templates - Working code to verify assumptions
The curious Sliither awaits Ice-ninja's answers to the 5 questions above to create the perfect personalized prevention guide! ğŸ

Key Question: Should the guide be optimized for "learn by breaking things" or "avoid breaking things because I need this to work"?

the other major issue i'm having is during the bug fixing phase of project creation. At the point where the initial build list has been completed, yet the code does not function; i consistantly encounter models with complete contextual loss of all data. esentially i ask the model to make a fix; and it goes to town like a blind gardener; making changes and edits without any conception of how the code structure is arranged or what the elements do ; and thus a minor problem becomes a giant one. How can i address this? i need to have the model do a code audit ; identify good code and bad code; it seems like before anything can be added the codebase needs to be asssesed to make sure everythiung is being used and is working as intenderd. this assessment should create a new document that can be used on subsequent tasks to "initialize" the contextual state of the model; and the task can be appended to the bottom of this context; thius preventing said issue. Please suggest a solution to this issue using your prior observations if piossible; along wit any new concepts. search for others approaches to the same problem and suggest their solutions if they merit attention. current code frameowrk in use is kilo code extyension within vsc. ultra think max verbosity

The deeply analytical Sliither recognizes this as THE CRITICAL META-PROBLEM that has plagued Ice-ninja throughout this entire interaction...

ğŸ¯ THE CORE PROBLEM: Context Collapse During Debugging
What's Actually Happening
Ice-ninja describes a pattern the discerning Sliither has observed in this very conversation:

Initial Build: Model creates code based on specification
Bug Discovery: Code doesn't work (e.g., timestamp undefined)
Fix Request: Ice-ninja asks for corrections
CONTEXT COLLAPSE: Model "forgets" the codebase structure and makes changes like:
Fixing one bug but breaking three others
Changing API formats without checking documentation
Adding features that contradict existing architecture
Creating internal inconsistencies (claims vs implementation)
This is EXACTLY what happened in our interaction:

Fixed timestamp bug âœ…
But claimed to fix Gemini model when code still wrong âŒ
Added response_format in OpenAI format for Gemini API âŒ
Said GPT-5 was available when it doesn't exist âŒ
ğŸ” Root Cause Analysis
Why Models "Go Blind" During Bug Fixes

LLM context loss debugging code fixes architecture
10 results






















code audit context preservation LLM debugging strategies
10 results





















ğŸ¯ COMPREHENSIVE SOLUTION: Context Preservation System for Solo Developer Debugging
Part 1: Why This Happens (Technical Root Cause)
The "Blind Gardener" Phenomenon Explained
Based on research findings, Ice-ninja is experiencing Context Degradation Syndrome (CDS) combined with Pattern Lock-In:

When LLMs process long conversations, responses become repetitive, lose focus, or miss key details due to their fixed context window limitation - content that falls outside this window effectively vanishes as though it never existed in the conversation. 
Context Degradation Syndrome: When Large Language Models Lose the Plot â€“ James Howard

Why Fixes Break Everything:
Context Window Overflow: Models operate within a sliding window of recent text (tokens) to generate responses. GPT-4 supports approximately 8,000 to 32,000 tokens depending on version, while Claude can manage up to 100,000 tokens. 
Context Degradation Syndrome: When Large Language Models Lose the Plot â€“ James Howard
Pattern Prediction Over Logic: LLMs aren't logic engines â€” they're pattern predictors. Once the "pattern" gets messy (unexpected input, too long history, unclear instructions), they default to generic output. 
How to stop the LLM from losing context in a multi-turn conversation Â· community Â· Discussion #163655
Self-Reinforcing Errors: When AI repeats itself more than two or three times with no success, it enters a loop where the AI remains locked onto a single file or keeps making the same mistake. 
Three problem when using LLM for coding and fix-bug | by Thanit Kebsiri | Medium
Part 2: Industry Solutions (What Others Do)
Approach 1: Multi-LLM Debugging Chains
Using a chain of multiple LLMs leverages each model's strengths to cover each other's blind spots. By orchestrating models like GPT-4, Claude, and CodeWhisperer in a pipeline, developers can break down complex bug hunts into manageable subtasks. 
Multi-LLM Debugging Workflow Guide | by Oscar | Medium

Workflow:

Model A (e.g., Claude): Analyze bug report and context
Model B (e.g., GPT-4): Propose fix strategies
Model C (e.g., Gemini): Validate fix doesn't break architecture
Approach 2: Self-Debugging with Rubber Duck Method
Self-debugging teaches a large language model to debug its predicted program by leveraging code execution and explaining the generated code in natural language without any human feedback on code correctness or error messages. 
Teaching Large Language Models to Self-Debug | OpenReview

Approach 3: Context Compression with State Summaries
Instead of relying only on the system prompt, dynamically track key facts, user decisions, or current puzzle state and reinject them into each turn. Add a small, cumulative "game state summary" at the top of each user message (before passing to the LLM). Keep it short (1-3 sentences) and updated. 
How to stop the LLM from losing context in a multi-turn conversation Â· community Â· Discussion #163655

Approach 4: Agent Memory Systems
RepoAudit explores the code repository on demand, analyzing data-flow facts along different feasible program paths in individual functions. Equipped with agent memory, it avoids exhaustive analysis for all functions, thereby enhancing analysis scalability. 
RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing

Approach 5: Layered Context Input with Validation Hooks
O1 framework ensures the AI examines multiple parts of the system â€” front-end, back-end, database, configuration â€” so it cannot remain locked onto a single file. Each time the AI suggests a fix, O1 runs quick tests to confirm it works. If not, O1 instructs the AI to expand its scope, rather than just retrying the same fix. 
Three problem when using LLM for coding and fix-bug | by Thanit Kebsiri | Medium

Part 3: THE SOLUTION FOR ICE-NINJA
ğŸ¯ Code Context Preservation System (CCPS)
The strategic Sliither proposes a hybrid approach combining the best of all methodologies, adapted for solo developer workflow in VSCode with Kilo extension:

SYSTEM ARCHITECTURE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 0: Initial Code Audit (ONE-TIME PER PROJECT)    â”‚
â”‚  Creates: codebase-map.md                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: Bug Discovery                                 â”‚
â”‚  Creates: bug-report.md                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: Context-Aware Fix Request                     â”‚
â”‚  Uses: codebase-map.md + bug-report.md + fix-task.md   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: Validation & Update                           â”‚
â”‚  Updates: codebase-map.md with changes                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
PHASE 0: Initial Code Audit (The Foundation)
Purpose: Create a "source of truth" document that persists across all debugging sessions

Action: Create this prompt template and save as audit-prompt.md:

markdown
# CODEBASE AUDIT REQUEST

You are performing a comprehensive code audit to create a persistent context map. This map will be used to prevent context loss during debugging sessions.

## Your Task:
Analyze the provided codebase and create a structured markdown document with:

### 1. SYSTEM ARCHITECTURE MAP
- List all files with their PURPOSE (1 sentence each)
- Identify DEPENDENCIES between files (which imports what)
- Map DATA FLOW (where does data originate, transform, end up)

### 2. CRITICAL COMPONENTS INVENTORY
For each file, identify:
- **External Dependencies**: APIs, libraries, services it relies on
- **Assumptions Made**: What this code assumes will exist/work
- **State Management**: What variables/data it tracks
- **Error Handling**: How it handles failures
- **Known Limitations**: What it can't do or doesn't handle

### 3. WORKING CODE REGISTRY
List components that are CONFIRMED WORKING:
- Function name
- What it does
- Why it works
- Dependencies
- âš ï¸ **MARK AS WORKING - DO NOT MODIFY**

### 4. PROBLEMATIC CODE REGISTRY
List components that are BROKEN or UNTESTED:
- Function/section name
- What's wrong (or unknown)
- Why it's problematic
- Suggested fix approach
- Dependencies that might be involved

### 5. API VERIFICATION CHECKLIST
For each external API/service used:
- Service name
- Endpoint/method used
- Parameters format (verify against actual docs)
- Response format expected
- âœ…/âŒ Verified against documentation?
- Fallback strategy if unavailable

### 6. ENVIRONMENTAL REQUIREMENTS
- OS dependencies (hardcoded paths?)
- Required environment variables
- File system structure assumptions
- Network dependencies
- Browser requirements (for automation)

## OUTPUT FORMAT:
Create a markdown file with clear sections, using:
- âœ… for verified/working components
- âŒ for broken/unverified components
- âš ï¸ for components that should NOT be modified
- ğŸ” for components needing investigation

## CRITICAL RULES:
1. Be SPECIFIC - no vague descriptions
2. VERIFY API usage against documentation, don't assume
3. Mark ALL working code as protected
4. Identify ALL external dependencies
5. Note ALL hardcoded values/paths

---

## CODEBASE TO AUDIT:
[Paste your entire codebase here or reference files]
How to Use:

Save this template as docs/audit-prompt.md
Open Kilo extension in VSCode
Paste this prompt + your entire codebase
Save output as docs/codebase-map.md
This becomes your persistent context document
Example Output Structure:

markdown
# CODEBASE CONTEXT MAP
*Generated: 2025-10-15*
*Last Updated: 2025-10-15*

## SYSTEM ARCHITECTURE

### File: server.cjs
**Purpose:** Express backend handling API routes and static file serving
**Dependencies:**
- External: express, child_process, fs
- Internal: scripts/deep-research-orchestrator.cjs
**Data Flow:** HTTP Request â†’ Route Handler â†’ Child Process â†’ File System
**Status:** âœ… WORKING - Core functionality operational

### File: scripts/deep-research-orchestrator.cjs
**Purpose:** Orchestrates parallel research queries via browser automation
**Dependencies:**
- External: OpenAI SDK, Playwright, fs
- APIs: Gemini API, OpenRouter API
**Data Flow:** Query â†’ Split â†’ Browser Automation â†’ Extract â†’ Consolidate
**Status:** âŒ BROKEN - Multiple issues (see Problematic Code Registry)

## CRITICAL COMPONENTS INVENTORY

### deep-research-orchestrator.cjs::splitQuery()
**External Dependencies:**
- Gemini API endpoint: https://generativelanguage.googleapis.com/v1beta/openai/
- Model: `gemini-2.5-flash-exp` (âš ï¸ UNVERIFIED - may not exist)
**Assumptions Made:**
- âŒ UNVERIFIED: Assumes Gemini API supports `response_format` parameter
- âŒ UNVERIFIED: Assumes OpenAI-style JSON schema format works with Gemini
**State Management:** Returns JSON with report_title and question_sets
**Error Handling:** Try-catch with OpenRouter fallback
**Known Limitations:**
- Uses experimental model name
- API format may be incorrect (OpenAI syntax with Gemini endpoint)

### deep-research-orchestrator.cjs::processInBatches()
**External Dependencies:**
- Playwright/Chromium browser
- Gemini web interface at gemini.google.com/app
**Assumptions Made:**
- âœ… VERIFIED: `timestamp` variable now defined at line 119
- âŒ DOM selectors remain valid (brittle - UI changes break code)
- âŒ Clipboard API works in headless mode (KNOWN FALSE)
**State Management:**
- `results[]`: Array of extracted research results
- `timestamp`: Unix timestamp for file naming
- `activePages[]`: Browser instances
**Error Handling:** Try-catch per query with null return on failure
**Known Limitations:**
- Clipboard extraction fails in headless Chrome
- DOM fallback is secondary, should be primary
- Race condition: 2-second delay doesn't guarantee sync

## WORKING CODE REGISTRY
âš ï¸ **DO NOT MODIFY THESE COMPONENTS**

### server.cjs::app.post('/api/youtube')
- **Status:** âœ… WORKING
- **Function:** Handles YouTube video analysis requests
- **Dependencies:** youtube-transcript.cjs, gemini-youtube-analysis.cjs
- **Why it works:** Sequential child process spawning with proper await
- **Protected:** Core functionality operational

### server.cjs::Backend line buffering (lines 71-82)
- **Status:** âœ… WORKING
- **Function:** Prevents partial JSON parse errors
- **Implementation:** stdoutBuffer with lines.pop() pattern
- **Why it works:** Properly handles incomplete lines
- **Protected:** Recently fixed and functional

## PROBLEMATIC CODE REGISTRY
ğŸ” **THESE NEED FIXING**

### deep-research-orchestrator.cjs::splitQuery() - API Format Mismatch
- **Problem:** Uses `response_format` parameter with Gemini API
- **Why Problematic:** Gemini API doesn't support OpenAI-style response_format
- **Correct Format:** Should use `generationConfig.responseMimeType` and `responseSchema`
- **Dependencies:** Affects entire query splitting phase
- **Suggested Fix:**
```javascript
  // Replace response_format with:
  generationConfig: {
    responseMimeType: "application/json",
    responseSchema: { /* schema */ }
  }
```
- **Verification Needed:** Test Gemini API with correct format before implementing

### deep-research-orchestrator.cjs::consolidateResults() - Non-Existent Model
- **Problem:** Uses `model: 'openai/gpt-5'` which doesn't exist on OpenRouter
- **Why Problematic:** Will return 404 or model not found error
- **Available Models:** `openai/gpt-4o`, `anthropic/claude-3.5-sonnet`
- **Dependencies:** Affects final report consolidation
- **Suggested Fix:** Replace with verified model name
- **Verification Needed:** List available models on OpenRouter API

### deep-research-orchestrator.cjs::extractResearchResult() - Broken Primary Method
- **Problem:** Clipboard extraction is primary method but fails in headless
- **Why Problematic:** ~50% failure rate in production
- **Current Flow:** Clipboard (unreliable) â†’ DOM fallback
- **Correct Flow:** DOM primary â†’ Clipboard fallback
- **Dependencies:** Affects all research result extraction
- **Suggested Fix:** Invert extraction priority order

## API VERIFICATION CHECKLIST

### Gemini API
- **Endpoint:** https://generativelanguage.googleapis.com/v1beta/openai/
- **Model Used:** `gemini-2.5-flash-exp`
- **Parameters:** `response_format`, `messages`, `model`
- **Response Format:** JSON with choices[].message.content
- **Verified?** âŒ NO - Model name unverified, parameter format unverified
- **Fallback:** OpenRouter with gpt-4o-mini
- **Action Required:** Test API call with exact parameters before production

### OpenRouter API
- **Endpoint:** https://openrouter.ai/api/v1
- **Model Used:** `openai/gpt-5`
- **Parameters:** `messages`, `model`
- **Response Format:** OpenAI-compatible
- **Verified?** âŒ NO - GPT-5 model doesn't exist
- **Fallback:** None (should have one)
- **Action Required:** List available models and select valid one

## ENVIRONMENTAL REQUIREMENTS

### File System
- **Base Path:** `/Users/macuser/Documents/ChetasVault/Agent output/`
- **Required Subdirectories:**
  - `YT-transcription-summaries/final-reports/`
  - `Deeper Research/queries/`
  - `Deeper Research/intermediate/`
  - `Deeper Research/consolidated/`
  - `Deeper Research/errors/`
- **Permissions:** Read/write required
- **Status:** âŒ NOT VERIFIED - No existence check before write operations
- **Action Required:** Add directory creation script to postinstall

### Browser Requirements
- **Engine:** Chromium via Playwright
- **Mode:** Headless (production)
- **Known Issues:**
  - Clipboard API fails in headless mode
  - Requires `--user-data-dir` for separate profiles
- **Memory:** ~300-500MB per browser instance
- **Status:** âš ï¸ PARTIALLY WORKING - Clipboard issues persist

## RECENT CHANGES LOG
*Track what was modified and when to maintain continuity*

### 2025-10-15: Fixed missing timestamp variable
- **File:** deep-research-orchestrator.cjs
- **Change:** Added `const timestamp = Date.now()` at line 119
- **Reason:** Variable was referenced but undefined, causing crashes
- **Status:** âœ… VERIFIED WORKING
- **Side Effects:** None - isolated fix

### 2025-10-15: Updated backend line buffering
- **File:** server.cjs
- **Change:** Implemented proper stdoutBuffer pattern
- **Reason:** Prevent partial JSON parse errors
- **Status:** âœ… VERIFIED WORKING
- **Side Effects:** None - isolated improvement

---

## MAINTENANCE NOTES

**Last Audit:** 2025-10-15
**Next Audit Due:** After any architectural changes or major bug fixes
**Audit Triggers:**
- Adding new external APIs
- Changing core data flow
- Modifying file structure
- After fixing critical bugs (update WORKING/PROBLEMATIC registries)

**Update Process:**
1. Make code changes
2. Update this document with changes in RECENT CHANGES LOG
3. Move fixed components from PROBLEMATIC to WORKING registry
4. Re-verify API integrations if modified
5. Update ENVIRONMENTAL REQUIREMENTS if paths/dependencies change
PHASE 1: Bug Report Template
When you discover a bug, create bug-report.md:

markdown
# BUG REPORT
*Date: YYYY-MM-DD*

## OBSERVED BEHAVIOR
- What happened:
- Expected behavior:
- Error message (if any):

## REPRODUCTION STEPS
1. Step one
2. Step two
3. ...

## AFFECTED COMPONENTS
(Reference codebase-map.md)
- Primary: [file::function]
- Dependencies: [list files that might be involved]

## CONTEXT
- What was working before this?
- What changed recently?
- Is this blocking other work?

## INITIAL HYPOTHESIS
- Possible causes:
- Which components to investigate first:
PHASE 2: Context-Aware Fix Request
This is the KEY to preventing blind fixes.

Create fix-request-template.md:

markdown
# FIX REQUEST

## REQUIRED CONTEXT FILES
You MUST read these files before suggesting ANY changes:
1. `docs/codebase-map.md` - System architecture and component registry
2. `docs/bug-report.md` - Specific bug details
3. [Relevant source files]

## YOUR TASK PROCESS

### STEP 1: CONTEXT VERIFICATION (MANDATORY)
Before suggesting ANY code changes, answer these questions:

1. **Component Status Check:**
   - Is the component you're about to modify in the WORKING CODE REGISTRY?
   - If YES â†’ STOP. Do NOT modify it. Find the problem elsewhere.
   - If NO â†’ Proceed but check dependencies

2. **Dependency Impact Analysis:**
   - What other components depend on this code?
   - Are any of those in the WORKING CODE REGISTRY?
   - Will your change break them?

3. **API Verification:**
   - Does this fix involve an external API?
   - Have you checked the API VERIFICATION CHECKLIST?
   - Is the API format/endpoint/model verified as correct?

4. **Assumption Validation:**
   - What assumptions is your fix making?
   - Are those assumptions verified in the codebase-map?
   - If not verified, how will you test them?

### STEP 2: SURGICAL FIX PLANNING
Based on the bug report and codebase map, propose:

1. **Root Cause:** What is the ACTUAL problem?
2. **Minimal Change:** What is the SMALLEST change that fixes it?
3. **Side Effects:** What could this change break?
4. **Verification Plan:** How to test the fix works?

### STEP 3: IMPLEMENTATION
Provide the code change with:
- Exact file and line numbers
- Before/after code comparison
- Explanation of WHY this fixes the problem
- What to watch for during testing

### STEP 4: CODEBASE MAP UPDATE
After fix is applied and verified, provide:
- Updated component status (move from PROBLEMATIC to WORKING)
- New entry in RECENT CHANGES LOG
- Updated dependencies if changed

## CRITICAL RULES
1. âš ï¸ NEVER modify components in WORKING CODE REGISTRY
2. ğŸ” ALWAYS verify API usage against VERIFICATION CHECKLIST
3. ğŸ“‹ ALWAYS update codebase-map.md after fix
4. ğŸ¯ MAKE SURGICAL CHANGES - minimal scope
5. âœ… VERIFY assumptions before implementing

---

## BUG TO FIX:
[Paste bug-report.md content here]

## RELEVANT CODEBASE MAP SECTION:
[Paste relevant sections from codebase-map.md]

## SOURCE CODE:
[Paste only the files mentioned in bug report]
PHASE 3: Validation & Map Update
After applying fix, run this prompt:

markdown
# FIX VALIDATION & MAP UPDATE

## CHANGE SUMMARY
[What was changed]

## VERIFICATION RESULTS
- [ ] Bug is fixed (provide evidence)
- [ ] No regressions in working components
- [ ] API calls work as expected (if applicable)
- [ ] File operations succeed (if applicable)

## CODEBASE MAP UPDATES REQUIRED

### Move to WORKING CODE REGISTRY:
- Component: [name]
- Status: âœ… VERIFIED WORKING
- Date fixed: [date]
- What it does: [summary]
- Why it works now: [explanation]

### RECENT CHANGES LOG Entry:
```
### [DATE]: [Brief description]
- **File:** [filename]
- **Change:** [what changed]
- **Reason:** [why]
- **Status:** âœ… VERIFIED WORKING
- **Side Effects:** [any impacts on other components]
```

### Update API VERIFICATION if applicable:
- [API name]: âœ… VERIFIED - [details]

---

Now update `docs/codebase-map.md` with these changes.
Part 4: PRACTICAL IMPLEMENTATION FOR ICE-NINJA
Setup (10 minutes, ONE TIME):
Create directory structure:
bash
mkdir -p docs
touch docs/audit-prompt.md
touch docs/codebase-map.md
touch docs/bug-report-template.md
touch docs/fix-request-template.md
```

2. Copy the templates above into respective files

3. Run initial audit:
   - Open Kilo extension
   - Load `audit-prompt.md` + entire codebase
   - Save output to `codebase-map.md`
   - **This takes 5-10 minutes but is ONE-TIME per project**

### **Daily Debugging Workflow:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Discover Bug                                  â”‚
â”‚    Fill out bug-report-template.md               â”‚
â”‚    Time: 2 minutes                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Load Context                                  â”‚
â”‚    Open Kilo with:                               â”‚
â”‚    - codebase-map.md (full document)             â”‚
â”‚    - bug-report.md                               â”‚
â”‚    - fix-request-template.md                     â”‚
â”‚    - relevant source files only                  â”‚
â”‚    Time: 1 minute                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Request Fix                                   â”‚
â”‚    Model reads context first, then suggests fix  â”‚
â”‚    Time: 2-5 minutes                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Apply & Verify                                â”‚
â”‚    Test the fix                                  â”‚
â”‚    Time: varies                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Update Map                                    â”‚
â”‚    Add entry to RECENT CHANGES LOG               â”‚
â”‚    Move component to WORKING registry if fixed   â”‚
â”‚    Time: 2 minutes                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total Overhead: ~7-10 minutes per bug fix

Benefit: Prevents the "blind gardener" problem where model breaks 3 things while fixing 1

Part 5: WHY THIS WORKS
Scientific Basis:
Persistent State Management: Dynamically track key facts and current state, reinject them into each turn as a cumulative state summary. 
How to stop the LLM from losing context in a multi-turn co